{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用Ollama模型\n",
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from typing import TypedDict\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "#https://smith.langchain.com/o/d1c9fe9f-3a8e-5df2-812b-f1dbc714d565/projects/p/479b7e6d-598a-4a5f-9a5c-515d7c50545d?timeModel=%7B%22duration%22%3A%227d%22%7D\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\" #trace\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "#https://app.tavily.com/?code=gWC496d2-HPgVCefwjLyPq9O2JKr8D1E9Wr9T--frPf-h&state=eyJyZXR1cm5UbyI6Imh0dHBzOi8vYXBwLnRhdmlseS5jb20ifQ\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\" #web search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swrd/iwb_proj/iwb/NLP_API/ting/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/swrd/iwb_proj/iwb/NLP_API/ting/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 初始化語言模型和嵌入模型並建立RAG\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0.6)\n",
    "\n",
    "# 加載Chroma\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda:0'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "#關於英濟\n",
    "Profile_directory = 'DB_En_add_tw/MGF_Profile'\n",
    "Profile_vectordb = Chroma(persist_directory=Profile_directory, embedding_function=embedding_model)\n",
    "Profile_retriever = Profile_vectordb.as_retriever(search_kwargs={\"k\": 10})\n",
    "#英濟事業\n",
    "Product_directory = 'DB_En_add_tw/MGF_Product'\n",
    "Product_vectordb = Chroma(persist_directory=Product_directory, embedding_function=embedding_model)\n",
    "Product_retriever = Profile_vectordb.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "#技術發展\n",
    "Technology_directory = 'DB_En_add_tw/MGF_Technology'\n",
    "Technology_vectordb = Chroma(persist_directory=Technology_directory, embedding_function=embedding_model)\n",
    "Technology_retriever = Technology_vectordb.as_retriever(search_kwargs={\"k\": 10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###建立要從vectorstore還是web_search的判斷器\n",
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
    "    )\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)    \n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"\n",
    "         You are an expert responsible for routing user questions to either a vectorstore or web search. \n",
    "         The vectorstore contains documents related to Megaforce, including information on its business and technological developments. \n",
    "         For questions on these topics, use the vectorstore. Otherwise, use web search.\n",
    "         \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "\n",
    "### 檢索評分器\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"\n",
    "         You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "         If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "         It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "         Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "         \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "###Generate\n",
    "Generate_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"\n",
    "         You are an assistant for question-answering tasks.\n",
    "         Use the following pieces of retrieved context to formulate your answer.\n",
    "         If the context does not provide sufficient information to answer the question, clearly state that you don't know.\n",
    "         Ensure your response includes at least one explanatory sentence or piece of information.\n",
    "         Generate the answer in a professional customer service style.\n",
    "         Question: {question}\n",
    "         Context: {context}\n",
    "         Please respond in either English or Traditional Chinese, choosing just one language:\n",
    "         \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = Generate_prompt | llm | StrOutputParser()\n",
    "\n",
    "###幻覺評分器\n",
    "class GradeHallucinations(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_Hall = llm.with_structured_output(GradeHallucinations)\n",
    "# Prompt\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"\n",
    "         You are a grader assessing whether an LLM-generated response is reasonably consistent with or loosely supported by a set of retrieved facts.\n",
    "         Provide a binary score: 'yes' or 'no'.\n",
    "         A 'yes' indicates that the response is generally plausible based on the facts, even if there are significant discrepancies, omissions, or speculative assumptions, as long as the overall message is coherent and not blatantly contradicted by the facts.\n",
    "         \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_Hall\n",
    "\n",
    "###回答評分器\n",
    "class GradeAnswer(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "structured_llm_Ans = llm.with_structured_output(GradeAnswer)\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"\n",
    "         You are a grader assessing whether an answer sufficiently addresses or relates to a question. \n",
    "         The bar is relaxed, so as long as the answer is somewhat relevant or provides partial resolution, \n",
    "         give a binary score 'yes'. A 'yes' means that the answer resolves or partially addresses the question. \n",
    "         Only give 'no' if the answer is completely off-topic or irrelevant.\n",
    "         \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_Ans\n",
    "\n",
    "### Question Re-writer\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"\"\"\n",
    "         You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "         for vectorstore retrieval. \n",
    "         Please rewrite the input question into an optimized version that is more suitable for vectorstore retrieval, while retaining the original question.\n",
    "         Observe the input content and try to infer its underlying semantic intent/meaning.\n",
    "         \"\"\"\n",
    "        ),\n",
    "        (\"human\",\"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    transform_query_count:int\n",
    "    previous_node: str\n",
    "    \n",
    "#Retrieve documents\n",
    "def retrieve(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    previous_node = state.get(\"previous_node\")\n",
    "    if previous_node:\n",
    "        if previous_node == \"Profile_retrieve\":\n",
    "            retriever = Profile_retriever\n",
    "        elif previous_node == \"Product_retrieve\":\n",
    "            retriever = Product_retriever\n",
    "        elif previous_node == \"Technology_retrieve\":\n",
    "            retriever = Technology_retriever\n",
    "    else:  # 如果 previous_node 為 None\n",
    "        retriever, previous_node = select_retriever(question)\n",
    "    \n",
    "    if retriever is None:\n",
    "        # 如果沒有找到合適的 retriever，直接返回並跳轉到 'web_search'\n",
    "        return {\"next_node\": \"web_search\", \"question\": question, \"previous_node\": \"web_search\"}\n",
    "    \n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question, \"previous_node\": previous_node}\n",
    "\n",
    "def select_retriever(question):\n",
    "    # 根據問題內容選擇檢索器\n",
    "    if \"profile\" in question.lower():\n",
    "        print(\"---PROFILE RETRIEVER SELECTED---\")\n",
    "        return Profile_retriever, \"Profile_retrieve\"\n",
    "    \n",
    "    elif \"product\" in question.lower():\n",
    "        print(\"---PRODUCT RETRIEVER SELECTED---\")\n",
    "        return Product_retriever, \"Product_retrieve\"\n",
    "    \n",
    "    elif \"technology\" in question.lower():\n",
    "        #print(\"---TECHNOLOGY RETRIEVER SELECTED---\")\n",
    "        return Technology_retriever, \"Technology_retrieve\"\n",
    "    else:\n",
    "        # 當沒有適合的檢索器時，返回 'web_search'\n",
    "        print(\"---NO SUITABLE RETRIEVER FOUND, RETURNING TO WEB SEARCH---\")\n",
    "        return None, \"web_search\"\n",
    "\n",
    "#Generate answer\n",
    "def generate(state):\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation with max_tokens for OllamaFunction\n",
    "    generation = rag_chain.invoke({\"context\": documents,\"question\": question })\n",
    " \n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "#評估檢索到的文檔是否與給定問題相關\n",
    "def grade_documents(state):\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "#Transform the query to produce a better question\n",
    "def transform_query(state):\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    query_count = (state.get('transform_query_count', 0) or 0) + 1\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question, \"transform_query_count\":query_count}\n",
    "\n",
    "#web Search\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "def web_search(state):\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    previous_node = state.get(\"previous_node\", \"web_search\")\n",
    "    print(question)\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question, \"previous_node\": previous_node}\n",
    "#Edges\n",
    "#選擇web search or RAG\n",
    "def route_question(state):\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "    \n",
    "# transform_query完後決定跳哪個節點   \n",
    "def decide_node(state):\n",
    "    print(\"---DECIDING NEXT NODE---\")\n",
    "    # 檢查上一次的節點\n",
    "    previous_node = state.get(\"previous_node\")\n",
    "    if previous_node == \"web_search\":\n",
    "        print(\"Previous node was web_search, returning to web_search.\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"Previous node was not web_search, proceeding to retrieve.\")\n",
    "        return \"retrieve\"\n",
    "    \n",
    "#生成或重新詢問    \n",
    "def decide_to_generate(state):\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    transform_query_count = state.get(\"transform_query_count\", 0)\n",
    "    filtered_documents = state[\"documents\"]\n",
    "    if transform_query_count <=5:\n",
    "        if not filtered_documents:\n",
    "            # All documents have been filtered check_relevance\n",
    "            # We will re-generate a new query\n",
    "            print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
    "            return \"transform_query\"\n",
    "        else:\n",
    "            # We have relevant documents, so generate answer\n",
    "            print(\"---DECISION: GENERATE---\")\n",
    "            return \"generate\"\n",
    "    else:\n",
    "        print(\"---QUERY COUNT OVER 5, SO DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "    \n",
    "#確定生成的內容是否基於文檔並回答了問題\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    # 檢查documents是否可以迭代\n",
    "    if hasattr(documents, '__len__'):\n",
    "        for i, doc in enumerate(documents):\n",
    "            print(f\"Processing document {i + 1} of {len(documents)}\")\n",
    "            # 單個文件和生成的回答配對進行評估\n",
    "            score = hallucination_grader.invoke({\"documents\": [doc], \"generation\": generation})\n",
    "            grade = score.binary_score\n",
    "            \n",
    "            # Check hallucination\n",
    "            if grade == \"yes\":\n",
    "                print(\"---決策: 生成內容是基於文件的---\")\n",
    "                # Check question-answering\n",
    "                print(\"---評分生成內容 vs 問題---\")\n",
    "                score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "                grade = score.binary_score\n",
    "                \n",
    "                if grade == \"yes\":\n",
    "                    print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "                    return \"useful\"\n",
    "                else:\n",
    "                    print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "                    return \"not useful\"\n",
    "\n",
    "        # 如果遍歷完所有文檔後仍未返回，則表示所有文檔都未通過幻覺評分\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN THIS DOCUMENT, CONTINUING---\")\n",
    "        return \"not supported\"\n",
    "    else:\n",
    "        score = hallucination_grader.invoke({\"documents\": [documents], \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        # Check hallucination\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "            # Check question-answering\n",
    "            print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "            score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "            grade = score.binary_score\n",
    "            if grade == \"yes\":\n",
    "                print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "                return \"useful\"\n",
    "            else:\n",
    "                print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "                return \"not useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "            return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"web_search\", web_search)  \n",
    "workflow.add_node(\"retrieve\", retrieve)  \n",
    "workflow.add_node(\"grade_documents\", grade_documents)  \n",
    "workflow.add_node(\"generate\", generate) \n",
    "workflow.add_node(\"transform_query\", transform_query) \n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },)\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    lambda state: state.get(\"next_node\", \"grade_documents\"),  # 檢查下一個節點\n",
    "    {\n",
    "        \"web_search\": \"web_search\",  # 如果返回的 next_node 是 'web_search'，則跳轉到 web_search\n",
    "        \"grade_documents\": \"grade_documents\",  # 否則繼續流程\n",
    "    })\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },)\n",
    "workflow.add_conditional_edges(\n",
    "    \"transform_query\",\n",
    "    decide_node,\n",
    "    {\n",
    "        \"retrieve\" : \"retrieve\",\n",
    "        \"web_search\": \"web_search\"\n",
    "    }, )\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },)\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "def get_new_model_instance():\n",
    "    # 每次提問前重新初始化模型\n",
    "    return workflow.compile()\n",
    "history = []\n",
    "# Run\n",
    "input_text = input('>>> ')\n",
    "history = []\n",
    "while input_text.lower() != 'bye':\n",
    "    app = get_new_model_instance()  # 重置模型上下文\n",
    "    inputs = {\"question\": input_text}\n",
    "    # 模型輸出\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():            \n",
    "            generation = value.get(\"generation\", \"No response generated.\")\n",
    "    history.append({\"question\": input_text, \"answer\": generation})\n",
    "    input_text = input('>>> ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 1 of 7\n",
      "binary_score='no'\n",
      "Processing document 2 of 7\n",
      "binary_score='yes'\n",
      "Processing document 3 of 7\n",
      "binary_score='yes'\n",
      "Processing document 4 of 7\n",
      "binary_score='yes'\n",
      "Processing document 5 of 7\n",
      "binary_score='yes'\n",
      "Processing document 6 of 7\n",
      "binary_score='yes'\n",
      "Processing document 7 of 7\n",
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# 從輸入資料中獲取文件和生成內容\n",
    "documents = [\n",
    "    Document(page_content='技術研發\\nTechnology R & D\\n創新研發中心\\n創新研發中心支援產業生態圈\\n2024/9/12 上午9:13\\n英濟 - 技術研發\\nhttps://www.megaforce.com.tw//zh-tw/Tech/Technology\\n1/6\\n'),\n",
    "    Document(page_content='more\\n完整模修能力\\n先進高精密模具製造\\n高品質精密量測設備\\n高精密模具加工設備\\n優異品質掌控設備\\n高端成型技術\\n高端成型技術\\n除各類精密塑膠零組件設計與射出成型外，英濟亦持續發展高值LSR液態矽\\n膠成型，不僅是單獨矽膠成型與矽塑膠結合成型量產，更進一步發展矽膠金\\n屬結合成型技術及矽膠雙射成型技術，而矽膠包覆中空成型技術更是領先業\\n界的創新開發。\\n順應愈趨多元的產品應用場景與需求，英濟將技術領域擴展至各類多元材質\\n與異材質結合等層面，協助客戶開發更多元新商品的可能性。\\n2024/9/12 上午9:13\\n英濟 - 技術研發\\nhttps://www.megaforce.com.tw//zh-tw/Tech/Technology\\n3/6\\n'),\n",
    "    Document(page_content='技術研發\\nTechnology R & D\\nInnovative R&D Centers\\nInnovative R&D Centers\\nSupport Industrial Ecosystem\\n2024/9/12 上午9:16\\nMEGAFORCE - Technology\\nhttps://www.megaforce.com.tw//en-global/Tech/Technology\\n1/6\\n'),\n",
    "    Document(page_content='more\\n選擇性膠黏劑、光固化接著劑\\n表面處理劑(Primer)\\n導熱材料(墊片與膠)\\n異材質結合技術\\n高分子實驗室\\n高分子實驗室\\n高分子實驗室成立於2002年，配備各類先進精密儀器，如：GC-MS、\\nICP、DSC、SEM等。實驗室管理完善、檢測能力不斷提升，已經形成了完\\n整有效地管理體系，並于2007年通過了符合CNAS（ISO 17025）檢測和校\\n準實驗室能力認可準則的管理體系。\\n2024/9/12 上午9:13\\n英濟 - 技術研發\\nhttps://www.megaforce.com.tw//zh-tw/Tech/Technology\\n5/6\\n'),\n",
    "    Document(page_content='more\\n雙/多射出成型技術\\n塑件包覆LSR成型\\n液態矽膠(LSR)成型/多層複合成型技術\\n精密埋射技術\\n金屬包覆LSR成型\\n材料應用技術\\n材料應用技術\\n為了更完善應對產品生產過程中的影響因子，英濟亦對材料與各製程中所需\\n技術積極投入研發，包含表面質感、材質結合、導熱技術等，結合數十項專\\n利構築的技術優勢，創造具差異化的競爭力，提供客戶創新材料的資源應\\n用，促使其發展更全面應用。\\n矽膠手感噴塗劑\\n2024/9/12 上午9:13\\n英濟 - 技術研發\\nhttps://www.megaforce.com.tw//zh-tw/Tech/Technology\\n4/6\\n'),\n",
    "    Document(page_content='more\\nService@megaforce.com.tw\\n新北市土城區自強街5號2樓\\n+886-2-2268-7790\\n2016 © MEGAFORCE. All Rights Reserved 法律聲明與使用條款 網站地圖\\n2024/9/12 上午9:13\\n英濟 - 技術研發\\nhttps://www.megaforce.com.tw//zh-tw/Tech/Technology\\n6/6\\n'),\n",
    "    Document(page_content='技術研發\\nTechnology R & D\\n高端成型技術\\n液態射出矽膠\\n英濟自行研發的選擇性黏著LSR(Liquid Silicone Rubber)，對PC塑料具\\n有良好的黏著性，且不黏金屬、不易沾模，適用於注射成型、熱壓成\\n型、異材質結合。產品精確度高，具優異的抗撕裂強度、回彈性、抗黃\\n變性、耐熱老化性、熱穩定性和耐候性，極具替代工程軟膠的潛力。材\\n料應用層面廣，涵蓋消費電子產品、戶外運動產品、攜帶型智慧裝置\\n等。集團擁有實驗室及研發團隊，可視客戶需求調整材料性能。\\n多射射出成型\\n在既有的射出成型機上，外掛一組~多組副射台，並搭配精密設計的模仁\\n旋轉機構，便可在一個行程中產出多色或異材結合的產品，滿足多變化\\n的工藝需求。 不僅大大的提升了不同材質間的結合強度，在於工程數的\\n減少，成型周期減少了，以往多段工程的數套模具，現在也只需要一套\\n設計完美的模具即可完成，因此模具數也減少了，成本也跟著降低。\\nLSR M+R射出成型\\n金屬材質應用於消費性電子產品日益增加，但因碰撞與摔落易造成外觀\\n損傷。藉由LSR成型技術，M+R(金屬+矽膠)設計可增加產品外觀與機構\\n防護。 雙材質設計，兼顧外觀質感與觸感，進一步提升產品價值。\\n2024/9/12 上午9:14\\n英濟 - 技術研發\\nhttps://www.megaforce.com.tw//zh-tw/Tech/Technology/List/1\\n1/2\\n')\n",
    "]\n",
    "\n",
    "\n",
    "generation = '根據提供的上下文，我們可以看到英濟公司的技術研發範圍非常廣泛，包括：\\n\\n* 高精密模具製造\\n* 精密量測設備\\n* 高精密模具加工設備\\n* 優異品質掌控設備\\n* 高端成型技術\\n* 液態矽膠(LSR)成型/多層複合成型技術\\n* 精密埋射技術\\n* 金屬包覆LSR成型\\n* 材料應用技術\\n* 矽膠手感噴塗劑\\n\\n另外，英濟公司還有高分子實驗室，配備先進精密儀器，如GC-MS、ICP、DSC、SEM等。實驗室管理完善，檢測能力不斷提升。\\n\\n在技術研發方面，英濟公司也發展了多射出成型技術，能夠在一個行程中產出多色或異材結合的產品。\\n\\n總之，英濟公司的技術研發有許多創新和先進的技術，涵蓋模具製造、精密量測、材料應用等各個方面。'\n",
    "\n",
    "# 依序處理每個文件\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Processing document {i+1} of {len(documents)}\")\n",
    "    \n",
    "    # 單個文件和生成的回答配對進行評估\n",
    "    score = hallucination_grader.invoke({\"documents\": [doc], \"generation\": generation})\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who is the CEO of Megaforce?(profile)\n",
      "A: Based on the provided context, it appears that WL_XU is listed as the Chairman and CEO of Megaforce. Therefore, I can inform you that WL_XU is the CEO of Megaforce.\n",
      "Q: When was Megaforce founded?(profile)\n",
      "A: I'd be happy to help you with that!\n",
      "\n",
      "Based on the provided context, I found information relevant to your optimized question.\n",
      "\n",
      "The company founding date of Megaforce is 1991. This information can be found in the context as \"1991\\n英濟香港有限公司成立。\" which translates to \"1991, 英濟香港有限公司 was founded.\"\n",
      "Q: 英濟公司的策略長是誰?(profile)\n",
      "A: 根據提供的資訊，英濟公司的策略長是趙晟。\n",
      "Q: 英濟公司的執行長是誰?(profile)\n",
      "A: 根據提供的組織架構資訊，英濟公司的執行長（董事長暨執行長）是徐文麟先生。\n",
      "Q: 英濟公司什麼時候成立的?(profile)\n",
      "A: 根據提供的內容，英濟公司於1991年成立。\n",
      "Q: 英濟公司的事業有什麼?(product)\n",
      "A: 根據提供的內容，我們可以知道英濟公司事業包括：\n",
      "\n",
      "* 精密塑膠零組件射出成型\n",
      "* 微精密結構模具開發\n",
      "* 電子組裝\n",
      "* 噴塗印刷等前中後段製程整合服務\n",
      "* 醫療級矽膠成型技術\n",
      "* 牙科機電整合產品\n",
      "* 微創手術器械產品\n",
      "* 醫療IoT整合產品\n",
      "* 藥械合一產品\n",
      "* 醫療教學課程\n",
      "* 光電事業，包括雷射應用技術、擴增實境(AR)顯示、高端醫學掃描以及數位牙科口腔建模技術\n",
      "\n",
      "這些都是英濟公司事業的範圍。\n",
      "Q: 介紹一下英濟公司的高精密塑膠暨矽膠成型事業?(product)\n",
      "A: 根據提供的背景資訊，英济公司的高精密塑膠暨矽膠成型事業主要涉及以下幾個方面：\n",
      "\n",
      "1. 精密塑膠零組件射出成型：英济公司擁有專業的塑膠零組件製造技術，能夠提供高精密度的塑膠零組件。\n",
      "2. 微精密結構模具開發：英济公司還具有微精密結構模具開發能力，能夠提供高精密度的模具設計和製造服務。\n",
      "3. 電子組裝、噴塗印刷等前中後段製程整合服務：英济公司除了提供塑膠零組件製造和微精密結構模具開發之外，還能夠提供電子組裝、噴塗印刷等前中後段製程整合服務。\n",
      "4. 液態矽膠(Liquid Silicone Rubber, LSR)成型技術：英济公司還投資了液態矽膠成型技術的研發和應用，這項技術能夠提供高精密度的矽膠零組件製造服務。\n",
      "5. 雙/多射出成型技術、UV黏著技術等先進技術應用：英济公司還積極投資了雙/多射出成型技術、UV黏著技術等先進技術的研發和應用，這些技術能夠提供高精密度和高品質的塑膠零組件製造服務。\n",
      "\n",
      "因此，英济公司的高精密塑膠暨矽膠成型事業主要涉及精密塑膠零組件射出成型、微精密結構模具開發、電子組裝等前中後段製程整合服務，以及液態矽膠成型技術和其他先進技術的研發和應用。\n",
      "Q: 介紹一下英濟公司的雷射光學事業事業?(product)\n",
      "A: 根據提供的文本內容，我們可以找到相關資訊。\n",
      "\n",
      "英錡科技（Megaforce）旗下的光電事業深耕雷射應用技術，有成就研發量產世界最小的雷射光機引擎。此外，該公司正在與策略合作夥伴共同開發擴增實境(AR)顯示、高端醫學掃描以及數位牙科口腔建模技術。\n",
      "\n",
      "基於這些資訊，我們可以描述英錡科技的激光光學業務產品特點和優勢如下：\n",
      "\n",
      "* 研發量產世界最小的雷射光機引擎\n",
      "* 擴增實境(AR)顯示、高端醫學掃描以及數位牙科口腔建模技術\n",
      "\n",
      "這些產品表現了英錡科技在激光光學業務方面的優秀技術和創新能力。\n",
      "Q: 英濟公司的技術研發有什麼?(technology)\n",
      "A: 根據提供的資訊，英濟公司的技術研發包括：\n",
      "\n",
      "* 高端成型技術：具有液態射出矽膠、選擇性黏著LSR等功能。\n",
      "* 多射射出成型：在既有的射出成型機上，外掛一組或多組副射台，並搭配精密設計的模仁旋轉機構，便可在一個行程中產出多色或異材結合的產品。\n",
      "* LSR M+R射出成型：金屬材質應用於消費性電子產品日益增加，但因碰撞與摔落易造成外觀損傷。藉由LSR成型技術，M+R(金屬+矽膠)設計可增加產品外觀與機構防護。\n",
      "* 精密埋射技術：可以在高精密度下完成射出成型和埋射。\n",
      "* 材料應用技術：包括表面質感、材質結合、導熱技術等。\n",
      "\n",
      "這些研發的技術都有助於提升產品的品質、強度和耐用性，並且能夠滿足多樣化的工藝需求。\n",
      "Q: 介紹一下英濟公司的高端成型技術?(technology)\n",
      "A: 根據提供的文獻內容，英濟公司擁有多項高端成型技術包括：\n",
      "\n",
      "1. 高值LSR液態矽膠成型：除了單獨矽膠成型與矽塑膠結合成型外，英濟還發展了矽膠金屬結合成型技術及矽膠雙射成型技術。\n",
      "2. 矽膠包覆中空成型技術：英濟領先業界的創新開發。\n",
      "3. 多元材質與異材質結合：英濟將技術領域擴展至各類多元材質與異材質結合等層面，協助客戶開發更多元新商品的可能性。\n",
      "\n",
      "此外，英濟公司還研發了許多其他高端成型技術，如：\n",
      "\n",
      "* 雙/多射出成型技術\n",
      "* 塑件包覆LSR成型\n",
      "* 液態矽膠(LSR)成型/多層複合成型技術\n",
      "* 精密埋射技術\n",
      "* 金屬包覆LSR成型\n",
      "\n",
      "英濟公司還對材料與各製程中所需技術積極投入研發，包含表面質感、材質結合、導熱技術等，結合數十項專利構築的技術優勢。\n",
      "Q: 介紹一下英濟公司的材料應用技術?(technology)\n",
      "A: 根據提供的文獻內容，我們可以看到英濟公司在材料應用技術方面有以下幾個突出之處：\n",
      "\n",
      "1. 高值LSR液態矽膠成型：英濟公司發展了高值LSR液態矽膠成型技術，除了單獨矽膠成型外，也可以進行矽塑膠結合成型量產。\n",
      "2. 矽膠金屬結合成型技術：英濟公司進一步發展了矽膠金屬結合成型技術，創新開發出新的產品應用可能性。\n",
      "3. 矽膠雙射成型技術：英済公司還發展出了矽膠雙射成型技術，提供客戶更廣泛的產品設計與製造選擇。\n",
      "4. 表面質感、材質結合、導熱技術等研發：英濟公司積極投入研發，包含表面質感、材質結合、導熱技術等，以創造具差異化的競爭力，並提供客戶創新材料的資源應用。\n",
      "5. 實驗室管理體系認可準則：高分子實驗室成立於2002年，配備先進精密儀器，如GC-MS、ICP、DSC、SEM等。該實驗室已經通過了符合CNAS（ISO 17025）檢測和校準實驗室能力認可準則的管理體系。\n",
      "\n",
      "綜上所述，英済公司在材料應用技術方面具有一系列先進且創新的技術，提供客戶廣泛的產品設計與製造選擇，並持續投入研發以保持競爭力。\n",
      "Q: 台北今天的天氣好不好?\n",
      "A: 根據提供的資訊，今日白天臺北市的天氣舒適至易中暑，高溫約為28-37度，有多雲，午後有局部短暫雷陣雨。因此，可以說今日台北的天氣相對炎熱和悶熱。\n"
     ]
    }
   ],
   "source": [
    "for entry in history:\n",
    "    print(f\"Q: {entry['question']}\")\n",
    "    print(f\"A: {entry['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
